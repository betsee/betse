# --------------------( LICENSE                            )--------------------
# Copyright 2014-2018 by Alexis Pietak & Cecil Curry.
# See "LICENSE" for further details.
#
# --------------------( SYNOPSIS                           )--------------------
# Project-wide GitLab-CI configuration, integrating the in-house free-as-in-beer
# continuous integration (CI) service exposed by GitLab with this project's
# "py.test"-driven test suite. GitLab-specific terminology used by this service
# includes (from highest- to lowest-level):
#
# * GitLab-CI, GitLab's high-level CI service coordinating project-specific and
#   shared Runners with projects - including project artefacts and metadata.
# * Runner, a low-level virtual machine conforming to the GitLab Runner API and
#   acquiring builds to process through the GitLab-CI coordinator API. There
#   exist two categories of Runners:
#   * Project Runners, hosted either directly by project maintainers on private
#     servers *OR* indirectly by for-profit intermediaries on private servers
#     paid for by the project maintainers. As the nomenclature implies, Project
#     Runners are specific to and hence accessible by only the project to which
#     they are assigned. In either case, "paid" and "private" are the general
#     keywords here. (Inapplicable for our use case.)
#   * Shared Runners. As the nomenclature implies, Shared Runners are accessible
#     to all projects on a GitLab instance. Hence, all projects hosted by the
#     official gitlab.com share access to the same Shared Runners. Shared
#     Runners are hosted either:
#     * If the GitLab instance hosting this project is *NOT* the official
#       gitlab.com, then by the prior mechanisms (e.g., by project maintainers
#       or for-profit intermediaries).
#     * If the GitLab instance hosting this project is the official gitlab.com,
#       then by the for-profit intermediary with whom gitlab.com has partnered:
#       as of this writing, DigitalOcean. In this and only this special
#       edge-case, Shared Runners are free-as-in-beer for both public and
#       private repositories hosted with this instance. An inevitable caveat, of
#       course, is that only Linux-based Shared Runners are available. To quote
#       the official response to a recent issue report requesting both OS X and
#       Windows support:
#       "If you are writing about shared runners on GitLab.com, then they are
#        only Linux based (with docker executor). If you need Windows, Mac or
#        any other OS for the runner, then you need to install it on your own
#        host and register it in your project on GitLab.com."
#
# This gitlab.com-hosted project has been configured to enable Linux-based
# Shared Runners. Exercising tests on either OS X or Windows requires doing so
# on an external third-party free-as-in-beer CI service specific to that
# platform and then integrating this service with GitLab.

#FIXME: Replace use of the default pip cache with "pip-accel", which appears to
#be substantially faster. For now, pip cache is better than no cache.

# ....................{ DOCKER                             }....................
# Colon-delimited name and tag of the first- or third-party Docker image
# registered with the Docker Hub Registery (e.g., "python:3", denoting the
# Docker image named "python" tagged as "3"), provisioning the scientific stack
# to be tested against. A tag is an alphanumeric label unique to an image,
# whose name is itself an alphanumeric label unique to the set of all images
# registered with the Docker Hub Registery. A tag typically specifies the
# version of that image to be used.
#
# For a list of all available Docker images, see the search bar at the top of
# "https://hub.docker.com". To find relevant images, consider (in order):
#
# * Either:
#   * Google "docker python3 matplotlib". Since Matplotlib transitively requires
#     most dependencies required by this project, this query (typically) yields
#     maximally relevant images.
#   * Search the Docker Hub Registry directly by:
#     * Switching the list box from its useless default of "All" to either
#       "Downloads" or "Stars", sorting hits on image usage or upvotes.
#     * Searching for "python". Unfortunately, since the search engine *ONLY*
#       searches image names rather than some combination of names,
#       descriptions, and/or Dockerfiles, the resulting hits tend to be only
#       minimally relevant.
# * For each image of interest, clicking the "Tags" subpage to list:
#   * All available tags for that image.
#   * For each such tag, the compressed filesize of that tagged image.
#
# All else being equal, the smallest image pre-packaging the largest number of
# dependencies required for our scientific stack is the most ideal. Note that
# downloading and installing dependencies via a package manager is significantly
# slower than merely downloading an image pre-packaging those dependencies.
#
# Docker official images are rumoured to be switching from an Ubuntu- to an
# Alpine Linux-based OS. Thanks to an obsessive-compulsive attention to
# minification, Alpine Linux is ideal for Docker-based CI. While Alpine Linux
# comes bundled with a package manager ("apk") providing a variety of scientific
# Python packages (e.g., "py-numpy"), these packages are all specific to the
# Python 2.7 ecosystem as of mid-2017. If and when Alpine Linux provides new
# Python 3.x-compatible scientific Python packages, the current choice of Docker
# image below should be revisited.
#
# For simplicity, we currently fallback to the official Anaconda 3 Docker image
# from Continuum Analytics. If and when Alpine Linux supports Python 3.x,
# consider a (probably painful and hence improbable) switch. See also:
#
# * Docker Hub Registery entry for this image:
#   https://hub.docker.com/r/continuumio/anaconda3
# * Open-source GitHub repository hosting this image's Dockerfile:
#   https://github.com/ContinuumIO/docker-images/tree/master/anaconda3
# * Platform-specific lists of all Anaconda packages installed by default:
#   http://repo.continuum.io/pkgs
# * A promising alternative installing additional optional dependencies over
#   Anaconda 3, including FFMpeg. It's fairly heavyweight (which is bad), but
#   frequently maintained (which is good):
#   https://hub.docker.com/r/kaggle/python
#   https://github.com/Kaggle/docker-python
# * A promising alternative layering Anaconda 3 onto Alpine Linux, thus
#   circumventing several of the aforementioned issues. Unfortunately, this
#   image is infrequently maintained and hence unreliable (which is terribad):
#   https://github.com/vishnu2kmohan/anaconda3-docker
image: continuumio/anaconda3

# ....................{ GLOBALS                            }....................
# Dictionary mapping from the name to value of each environment variable to be
# "globally" exported and hence accessible to *ALL* commands run below.
variables:
  # ...................{ GLOBALS ~ public                   }...................
  # Public environment variables specific to third-party applications.

  # Instruct Matplotlib to cache metadata to the build-relative directory
  # repeated in the "cache:" section below.
  MPLCONFIGDIR: "mpl-cache"

  # Instruct "pip" to cache all downloads to the build-relative directory
  # repeated in the "cache:" section below.
  # PIP_CACHE_DIR: "pip-cache"

  # ...................{ GLOBALS ~ private                  }...................
  # Private environment variables specific to this configuration. To avoid
  # conflict with third-party applications, the name of each such variable is
  # intentionally prefixed by "_".

  # Relative path of the top-level directory containing this configuration's
  # Ananconda environment repeated in the "cache:" section below. To
  # differentiate this environment from an environment of the same name in the
  # default system-wide directory for Anaconda environments (e.g.,
  # "/opt/conda/envs"), this path is intentionally prefixed by "./".
  _CONDA_ENV_DIRNAME: "./conda-env"


cache:
  # Enable per-branch caching, assigning each repository branch a unique cache.
  # By default, Gitlab-CI enables per-job and per-branch caching, assigning each
  # repository branch for each job a unique cache. While a sensible default,
  # this pipeline does *NOT* isolating each cache to each job. On the contrary,
  # the build and test jobs defined below expect to share the same cache.
  #
  # For a human-readable list of permissible values for this setting, see also:
  #     https://docs.gitlab.com/ce/ci/yaml/README.html#cache-key.
  key: "$CI_BUILD_REF_NAME"

  # Cache all subdirectories and files of the build directory *NOT* already
  # tracked by Git for this repository, in addition to those paths explicitly
  # cached below. In theory, all paths requiring caching should be explicitly
  # cached below; in practice, this fallback ensures that paths omitted below
  # will still be implicitly cached.
  untracked: true

  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  # WARNING: Due to an outstanding issue, GitLab-CI currently ignores *ALL*
  # cache paths outside the build directory. See also:
  #     https://gitlab.com/gitlab-org/gitlab-ce/issues/4431
  # Sadly, this implies that cache paths listed below *MUST* be both relative to
  # and contained in the build directory. Ensure that each such path is prefixed
  # by neither "/", "./", or "../" *OR* by any variable expanding to such a path
  # (e.g., "$HOME").
  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  #
  # List of the relative or absolute paths of all directories to be preserved
  # between CI pipelines. (Note that relative paths are relative to the current
  # build directory.)
  paths:
    # The directory to which Anaconda persists this configuration's environment.
    - conda-env/

    # The directory to which Matplotlib caches metadata (e.g., on fonts).
    - mpl-cache/

    # The directory to which pip caches artifacts pertaining to previously
    # installed and possibly compiled dependencies.
    # - pip-cache
    # - $HOME/.cache/pip

# ....................{ GLOBALS ~ script                   }....................
# List of all external commands run *BEFORE* running those listed by the
# "script" key of any job below. These commands build and install both this
# application and all third-party dependencies required by this application.
before_script:
  # Update all system packages installed by default with this image for the
  # duration of this CI pipeline.
  #
  # Since this image is frequently updated, this is ignored.
  #- apt-get update -qy

  # Install all dependencies available via the system-wide package manager,
  # which is typically both faster and stabler than doing so via pip3.
  #
  # Since this image provides such dependencies by default, this is ignored.
  # - apt-get install -y

  #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  # WARNING: Since this pipeline currently leverages the Anaconda-based "conda"
  # manager rather than the pure-Python "pip" manager to install dependencies,
  # the following command is vestigial and is preserved only for posterity.
  #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  # Install all Python-specific dependencies via "pip". Dismantled, this is:
  #
  # * "--quiet", reducing output verbosity.
  # * "--disable-pip-version-check", improving both space and time complexity
  #   by preventing "pip" from unnecessarily upgrading itself.
  # * "--cache-dir", redefining the same directory exported as an environment
  #   variable above. While redundant, doing so increases the likelihood that
  #   "pip" will actually respect this cache request.
  # * "--upgrade-strategy", improving both space and time complexity by
  #   preventing "pip" from unnecessarily upgrading dependencies already
  #   satisfying application requirements.
  # * "--requirement", the relative path of the file listing all dependencies
  #   to be installed.
  # - pip3 --quiet --disable-pip-version-check --cache-dir "${PIP_CACHE_DIR}" install --upgrade-strategy only-if-needed --requirement requirements.txt

  # Configure "conda" to run in headless mode. Dismantled, this is:
  #
  # * "always_yes true", automatically pass the "--yes" option to *ALL*
  #   "conda" commands run below. This is a superficial convenience reducing
  #   the likelihood of developer oversight and hence saving essential sanity.
  # * "auto_update_conda false", improving both space and time complexity by
  #   preventing "conda" from unnecessarily upgrading itself.
  - conda config --set always_yes true --set auto_update_conda false

  # Update "conda" *SAFELY.* To circumvent a well-known hard blocker in the
  # supposedly stable release of conda 4.4, "conda" is intentionally *NOT*
  # updated here with a standard one-liner: "conda upgrade -y conda". For
  # further details, see the following exhaustive GitHub issue:
  #     https://github.com/conda/conda/issues/6811
  - conda install --quiet --name base conda

  # For debuggability, print metadata identifying this Anaconda release.
  - conda info --all

  #FIXME: This doesn't appear to actually be cached. We have little idea why.
  #The "cache" section defined above appears to be syntactically correct and
  #indeed be caching the same directory defined below. The culprit, really, is
  #the complete (and utterly frustrating) lack of detailed debug output from
  #Gitlab-CI concerning which exact paths are being cached and when. Until
  #such output lands, we probably won't be able to reasonably address this.

  # Create an empty Anaconda environment if not already created by a prior job
  # *OR* silently ignore the resulting error otherwise. To create this
  # environment in a cachable build-local directory rather than the uncachable
  # default directory for Anaconda environments, the "--prefix" rather than
  # "--name" option is passed. Since these two options are mutually exclusive,
  # only the former is passed.
  #
  # Note that isolating the installation of package dependencies into this
  # environment is essential. Although this installation is already confined
  # to a temporary Docker image and hence requires no additional isolation,
  # Anaconda *ONLY* supports configurable caching via the "${CONDA_PREFIX}"
  # environment variable specific to Anaconda environments. To persist this
  # installation across jobs, there exists no sane alternative.
  - conda create --quiet --prefix "${_CONDA_ENV_DIRNAME}" || true

  # Activate this environment (i.e., prepend the current ${PATH} by this
  # environment's top-level directory).
  - source activate "${_CONDA_ENV_DIRNAME}"

  #FIXME: Uncomment *AFTER* the conda-forge variants of application dependencies
  #are sufficiently stable. As of this writing (i.e., 2018 Q2), one or more such
  #dependencies non-deterministically induce segmentation faults on running the
  #"test" job defined below under GitLab-CI (but, curiously, not Appveyor): e.g.,
  #
  #    $ betse --matplotlib-backend=agg info
  #    [betse] Welcome to <<BETSE 0.7.1 | CPython 3.6.2 | debian 9.4>>.
  #    [betse] Loading third-party BETSE dependencies...
  #    [betse] Harvesting system metadata... (This may take a moment.)
  #    /bin/bash: line 76:    70 Segmentation fault      (core dumped) betse --matplotlib-backend=agg info
  #    ERROR: Job failed: exit code 1
  #
  #Unfortunately, this implies that all optional dependencies installable *ONLY*
  #through conda-forge must be disabled in the "requirements-conda.txt" file.

  # Add the third-party "conda-forge" channel as the highest-priority channel,
  # ensuring that subsequently installed dependencies prefer the versions
  # supplied by this channel rather than any official default channels. This
  # is strongly preferable for any number of obvious reasons, including:
  #
  # * Official default channels either fail to:
  #   * Supply packages (e.g., "pyside2").
  #   * Supply working packages (e.g., "graphviz", whose official package
  #     appears to suffer numerous well-known deficiencies under Windows).
  # * Parity with BETSE's existing Anaconda package, which is hosted by
  #   conda-forge for the above reasons and more. Parity with respect to
  #   dependency installation increases the likelihood of parity between
  #   testing results and the end user experience.
  #- conda config --add channels conda-forge

  # Add the official "anaconda" channel as the highest-priority channel. This
  # channel provides only a subset of the optional dependencies provided by the
  # "conda-forge" channel, which is bad. Those dependencies that are provided
  # by this channel, however, are typically stabler than those provided by the
  # "conda-forge" channel, which is good. (The good outweighs the bad here.)
  - conda config --add channels anaconda

  # Install all mandatory and optional dependencies of this application into
  # this environment from the "conda-forge" channel.
  #
  # Note that passing the "--file requirements-conda.txt" option to the above
  # "conda create" command would suffice to do so for the initial creation of
  # this environment but fail to account for subsequent changes to these
  # dependencies; hence, environment creation and dependency installation
  # *MUST* be separated. The efficiency hit is minimal if any.
  #
  # Note also that, in theory, it would be superficially feasible to replace our
  # usage of this external requirements file with usage of the dependencies
  # already defined for BETSE's existing Anaconda package with the new "--only-deps"
  # option introduced with conda 4.4: e.g.,
  #
  #     - conda install --only-deps -c conda-forge betse
  #
  # In practice, that would be a terrible idea. There exists no guarantee that
  # the dependency list required by the most recent stable release of BETSE (and
  # hence that Anaconda package) perfectly coincides with the dependency list
  # required by the live version of BETSE exercised by this configuration.
  # Moreover, doing so would also install PySide2 -- a heavyweight interactive
  # dependency with no meaningful utility during testing.
  #
  # In short: "Explicit is still better than implicit."
  - conda install --quiet --file requirements-conda.txt

  # Install this project into this environment in the most efficient means
  # possible (i.e., without copying this project into this environment).
  - python setup.py develop

# ....................{ STAGES                             }....................
# List of all stages to be run by this pipeline (in order).
#
# In Gitlab-CI parlance, a "stage" is an abstract tag to which a "job" (defined
# below) is assigned. Each job is *ALWAYS* tagged with exactly one stage,
# defaulting to the "test" stage. All jobs tagged with the same stage are run in
# parallel *BEFORE* all jobs tagged with the next stage in this list are run in
# parallel. If any job fails, the entire stage to which that job belongs fails.
# If any stage fails, the entire pipeline fails; else, the pipeline succeeds.
#
# Previously, this pipeline listed the following two stages:
#
#     stages:
#       - build
#       - test
#
# These stages were immplemented by the following two jobs:
#
# * "betse_build", implementing the "build" stage by installing dependencies.
# * "betse_test", implementing the "test" stage by testing this application.
#
# Sadly, this seemingly reasonable partition of the pipeline workflow silently
# ceased working at some unidentifiable time in the development history. To
# remedy this, all work previously previously performed by the "betse_build"
# job is now performed as a global "before_script" key. While non-ideal, there
# appears to be no remedy as yet.
stages:
  # - build
  - test

# ....................{ JOBS                               }....................
# In Gitlab-CI parlance, a "job" is a container of all Gitlab-CI configuration
# metadata guaranteed to be enabled for the same duration of the pipeline time.
# This contradicts conventional *nix-oriented parlance, in which a "job" is
# simply a subprocess owned by a parent shell process.
#
# Each Gitlab-CI job is uniquely identified by a top-level user-defined key of
# this YAML file *NOT* already reserved for use as a top-level official key by
# the ".gitlab-ci.yml" file format (e.g., "artifact", "cache", "script"). A job
# may have any arbitrary non-reserved name and may contain any top-level
# official key, thus confining the action of that key (e.g., artifact building,
# caching, script commands) to that job.

# ....................{ JOBS ~ test                        }....................
# Test-specific job, exercising the entire test suite for this application.
betse_test:
  # Stage to run this job under. Note that "test" is technically the default
  # stage and hence need *NOT* be explicitly specified here. For disambiguity,
  # we do so anyway.
  stage: test

  # List of all external commands run by this job.
  script:
    #FIXME: For unknown reasons, this command causes spurious non-deterministic
    #segmentation faults on both Windows *AND* POSIX-compatible platforms. This
    #issue is almost certainly due to this command's attempt to unsafely
    #exercise interactive matplotlib backends in non-interactive CI
    #environments. For unknown reasons, the same command appears to succeed when
    #encapsulated by a py.test test. Until this issue is resolved, refer to the
    #logging output captured by the "test_cli_info" test.

    # For debuggability, print metadata identifying this BETSE release. Since
    # the same metadata is also printed, captured, and squelched by a functional
    # test exercised by testing run below, doing so is redundant. Nonetheless,
    # py.test provides no means of selectively disabling output capturing for
    # only specific tests. For efficiency and readability, globally disabling
    # output capturing is undesirable. Dismantled, this is:
    #
    # * "--matplotlib-backend=agg", preventing BETSE from erroneously attempting
    #   to initialize an interactive-only matplotlib backend (e.g., "PyQt4").
    #- betse --matplotlib-backend=agg info

    # Run the entire "py.test"-based test suite under the following options:
    #
    # * "--maxfail=3", halting testing on the third failure. For discussion, see
    #   the "betse_setup.test" submodule.
    - py.test --maxfail=3
    # - py.test --maxfail=3 -s
